para.store[[k]][i,"co"] <- sum(W==0)
}
est.summ <- lapply(est.store, function(x)round(apply(x, 2, mean, na.rm=T), 5))
para.summ <- lapply(para.store, function(x)round(apply(x, 2, mean, na.rm=T), 5))
mse <- lapply(est.store, function(x) apply(x, 2, function(y) mean((y-0)^2, na.rm=T)))
bias <- lapply(est.store, function(x) apply(x, 2, function(y) mean(y-0, na.rm=T)))
}
i
X13 <- mvrnorm(n,mu=mu,Sigma=Sigma, empirical = F)
X1 <- X13[,1]
X2 <- X13[,2]
X3 <- X13[,3]
X4 <- runif(n,-3,3)
X5 <- rchisq(n, df=1)
X6 <- rbinom(n,size=1,prob=.5)
e <- rnorm(n,mean=0,sd=sqrt(30))
xb <- X1 + 2*X2 - 2*X3  - X4 - .5*X5 + X6 +  e
W  <- as.numeric(xb>0)
# sample to target sizes
sim.dat <- data.frame(W,X1,X2,X3,X4,X5,X6)
tr.keep <- sample(which(sim.dat$W==1),tr,replace=F)
co.keep <- sample(which(sim.dat$W==0),co,replace=F)
sim.dat <- sim.dat[c(tr.keep,co.keep),]
W <- sim.dat[,"W"]
X <- as.matrix(sim.dat[,names(sim.dat)[-1]])
X1 <- X[,"X1"]; X2 <- X[,"X2"]; X3 <- X[,"X3"]; X4 <- X[,"X4"]; X5 <- X[,"X5"];X6 <- X[,"X6"]
PS.out  <- glm(W~X,family=binomial(link = "probit"))
PS.true <- PS.out$fitted
# propensity score specification formulae
psxlist <- pslist <- list()
# ps 1
formula.PS1 <- formula(W~X1+X2+X3+X4+X5+X6)
psxlist[[1]] <- model.matrix(formula.PS1)[,-1]
# ps 2
formula.PS2  <- formula(W~I(X1^2)+I(X2^2)+X3+I(X4^2)+I(X5^2)+X6)
psxlist[[2]] <- model.matrix(formula.PS2)[,-1]
# ps 3
formula.PS3  <- formula(W~I(X1*X3)+I(X2^2)+X4+X5+X6)
psxlist[[3]] <- model.matrix(formula.PS3)[,-1]
# PS estimation
for(p in 1:3){
pslist[[p]] <- glm(W~psxlist[[p]],family=binomial(link = "probit"))
}
# outcome: treatment effect is zero
u  <- rnorm(nrow(X))
# outcome 1 (linear)
Y1 <- X1 + X2 + X3 - X4 + X5 + X6 + u
# outcome 2 (medium nonlinear)
Y2 <- X1 + X2 + 0.2*X3*X4 - sqrt(X5) + u
# outcome 3 (strongly nonlinear)
Y3 <- (X1 + X2 + X5)^2 + u
# combine
Y.mat <- cbind(Y1,Y2,Y3)
for(k in 1:3){
Y <- Y.mat[,k]
est.store[[k]][i,"RAW"] <- mean(Y[W==1]) - mean(Y[W==0])
}
Y.mat
est.store[[k]][i,"RAW"]
est.store
sims <- 100
n    <- 5000
# X1-X3 multivariate normal with variances 2, 1, 1 and covars 1, -1, -.5
vars   <- c(2,1,1)
covars <- c(1,-1,-.5)
mu     <- c(0,0,0)
Sigma <- diag(vars)
Sigma[2,1] <- Sigma[1,2] <- covars[1]
Sigma[3,1] <- Sigma[1,3] <- covars[2]
Sigma[3,2] <- Sigma[2,3] <- covars[3]
# pick target sample size
#nsim <- 1500
#nsim <- 600
nsim  <- 300
tr <- nsim/2; tr
co <- nsim - tr; co
para  <- c("FracCtoT","N","tr","co","corY.PS","corW.PS","corW.MM1","corW.MM2","corW.MM3","corPS.PS1","corPS.PS2","corPS.PS3")
est    <- c("RAW","MM1","MM1ATT","MM2","MM2ATT","MM3","MM3ATT","MM4","MM4ATT","PS1","PS2","PS3","PSW1","PSW2","PSW3","EB","GM")
# expand storage for three different outcomes
est.store <- para.store <- list()
for(k in 1:3){
est.store[[k]]   <- matrix(NA,sims,length(est), dimnames = list(c(), est))
para.store[[k]]  <- matrix(NA,sims,length(para), dimnames = list(c(), para))
}
names(est.store) <- names(para.store) <- c("Y1","Y2","Y3")
for(i in 1:10){ # start simulations
print(i)
# draw Xs
X13 <- mvrnorm(n,mu=mu,Sigma=Sigma, empirical = F)
X1 <- X13[,1]
X2 <- X13[,2]
X3 <- X13[,3]
X4 <- runif(n,-3,3)
X5 <- rchisq(n, df=1)
X6 <- rbinom(n,size=1,prob=.5)
e <- rnorm(n,mean=0,sd=sqrt(30))
xb <- X1 + 2*X2 - 2*X3  - X4 - .5*X5 + X6 +  e
W  <- as.numeric(xb>0)
# sample to target sizes
sim.dat <- data.frame(W,X1,X2,X3,X4,X5,X6)
tr.keep <- sample(which(sim.dat$W==1),tr,replace=F)
co.keep <- sample(which(sim.dat$W==0),co,replace=F)
sim.dat <- sim.dat[c(tr.keep,co.keep),]
W <- sim.dat[,"W"]
X <- as.matrix(sim.dat[,names(sim.dat)[-1]])
X1 <- X[,"X1"]; X2 <- X[,"X2"]; X3 <- X[,"X3"]; X4 <- X[,"X4"]; X5 <- X[,"X5"];X6 <- X[,"X6"]
# true linear propensity scores
PS.out  <- glm(W~X,family=binomial(link = "probit"))
PS.true <- PS.out$fitted
# propensity score specification formulae
psxlist <- pslist <- list()
# ps 1
formula.PS1 <- formula(W~X1+X2+X3+X4+X5+X6)
psxlist[[1]] <- model.matrix(formula.PS1)[,-1]
# ps 2
formula.PS2  <- formula(W~I(X1^2)+I(X2^2)+X3+I(X4^2)+I(X5^2)+X6)
psxlist[[2]] <- model.matrix(formula.PS2)[,-1]
# ps 3
formula.PS3  <- formula(W~I(X1*X3)+I(X2^2)+X4+X5+X6)
psxlist[[3]] <- model.matrix(formula.PS3)[,-1]
# PS estimation
for(p in 1:3){
pslist[[p]] <- glm(W~psxlist[[p]],family=binomial(link = "probit"))
}
# outcome: treatment effect is zero
u  <- rnorm(nrow(X))
# outcome 1 (linear)
Y1 <- X1 + X2 + X3 - X4 + X5 + X6 + u
# outcome 2 (medium nonlinear)
Y2 <- X1 + X2 + 0.2*X3*X4 - sqrt(X5) + u
# outcome 3 (strongly nonlinear)
Y3 <- (X1 + X2 + X5)^2 + u
# combine
Y.mat <- cbind(Y1,Y2,Y3)
# raw outcome
for(k in 1:3){
Y <- Y.mat[,k]
est.store[[k]][i,"RAW"] <- mean(Y[W==1]) - mean(Y[W==0])
}
# Model matching predictions
mmxlist <- mmlist <- list()
for(k in 1:3){
Y <- Y.mat[,k]
# Model 1: simple linear regression on all covariates
model.MM1 <- lm(Y~X1+X2+X3+X4+X5+X6)
mmxlist[[1]] <- model.MM1$fitted
# Model 2: linear regression on all covariates with higher order polynomial terms
model.MM2 <- lm(Y~X1+I(X1^2)+I(X1^3)+X2+I(X2^2)+I(X2^3)+X3+I(X3^2)+I(X3^3)+X4+I(X4^2)+I(X4^3)+X5+I(X5^2)+I(X5^3)+X6+I(X6^2)+I(X6^3))
mmxlist[[2]] <- model.MM1$fitted
# Model 3: bagged regression trees, default 25 bags
model.MM3 <- bagging(Y~., data = data.frame(Y,X), coob=TRUE)
mmxlist[[3]] <- predict(model.MM3, X)
# Model 4: random forest
model.MM4 <- randomForest(Y~., data = data.frame(Y,X))
mmxlist[[4]] <- predict(model.MM4, X)
# combine
mmlist[[k]] <- mmxlist
}
# model-based matching
for(k in 1:4){
Y <- Y.mat[,k]
for(p in 1:4){
pred <- mmlist[[k]][[p]]
pairs <- Matches(W, pred)
pairs.df <- data.frame(do.call(rbind,pairs))
w0 <- 1/table(pairs.df[pairs.df$tr == 0,1]); Y0 <- Y[unique(pairs.df[pairs.df$tr==0,1])]
mm_res <- permu_test_mean(groups = pairs, prediction = pred, treatment = W, response = Y, iters=1000)
est.store[[k]][i,paste("MM", p, sep="")] <- mm_res[[1]] #mm_res[[1]]/tr # edit: I changed MM function to divide by number treated
est.store[[k]][i,paste("MM", p, "ATT", sep="")] <- mean(Y[W==1]) - sum(Y0*w0)/sum(w0)
}}
# entropy balancing
for(k in 1:3){
Y <- Y.mat[,k]
out.eb <- try(ebalance(
Treatment=W,
X=X,
print.level = -1
),silent=FALSE)
if(class(out.eb)=="try-error"){
cat("EB did not converge in sims",i,"\n")
} else {
d <- out.eb$w
est.store[[k]][i,"EB"]  <- mean(Y[W==1]) - (sum(Y[W==0]*d)/sum(d))
}}
## GenMatching
for(k in 1:3){
Y <- Y.mat[,k]
suppressWarnings(gout <- GenMatch(Tr=W, X=X, BalanceMatrix=X, estimand="ATT", M=1, weights=NULL,
print.level=0))
out  <- Match(Tr=W,X=X,Weight.matrix=gout,estimand="ATT",M=1,BiasAdjust=FALSE)
est.store[[k]][i,"GM"] <- (sum(Y[out$index.treated]*out$weights)/sum(out$weights)) - (sum(Y[out$index.control]*out$weights)/sum(out$weights))
}
# Propensity scoring methods
# Matching
for(k in 1:3){
Y <- Y.mat[,k]
for(p in 1:3){
if(sum(pslist[[p]]$fitted<(0+sqrt(.Machine$double.eps)))> 0 || sum(pslist[[p]]$fitted>(1-sqrt(.Machine$double.eps)))>0){ # error reporting if perfect sep
cat("Perfect Seperation in glm occured in sim",i,"\n")
} else {
# ps matching
PS <- pslist[[p]]$linear
out <- Match(Tr=W,X=PS,estimand="ATT",M=1,BiasAdjust = FALSE)
est.store[[k]][i,paste("PS",p,sep="")] <- (sum(Y[out$index.treated]*out$weights)/sum(out$weights)) - (sum(Y[out$index.control]*out$weights)/sum(out$weights))
}
# Weighting
PS.pr <- pslist[[p]]$fitted
d <- PS.pr/(1-PS.pr)
est.store[[k]][i,paste("PSW",p,sep="")] <- mean(Y[W==1])- (sum(Y[W==0]*d[W==0])/sum(d[W==0]))
}
# store housekeeping vars
para.store[[k]][i,"FracCtoT"] <- sum(W==0)/sum(W==1)
para.store[[k]][i,"corY.PS"] <- cor(Y,PS.true)
para.store[[k]][i,"corW.PS"] <- cor(W,PS.true)
para.store[[k]][i,"corW.MM1"] <- cor(W, mmlist[[k]][[1]])
para.store[[k]][i,"corW.MM2"] <- cor(W, mmlist[[k]][[2]])
para.store[[k]][i,"corW.MM3"] <- cor(W, mmlist[[k]][[3]])
para.store[[k]][i,"corPS.PS1"] <- cor(pslist[[1]]$fitted,PS.true)
para.store[[k]][i,"corPS.PS2"] <- cor(pslist[[2]]$fitted,PS.true)
para.store[[k]][i,"corPS.PS3"] <- cor(pslist[[3]]$fitted,PS.true)
para.store[[k]][i,"N"]  <- nrow(sim.dat)
para.store[[k]][i,"tr"] <- sum(W==1)
para.store[[k]][i,"co"] <- sum(W==0)
}
est.summ <- lapply(est.store, function(x)round(apply(x, 2, mean, na.rm=T), 5))
para.summ <- lapply(para.store, function(x)round(apply(x, 2, mean, na.rm=T), 5))
mse <- lapply(est.store, function(x) apply(x, 2, function(y) mean((y-0)^2, na.rm=T)))
bias <- lapply(est.store, function(x) apply(x, 2, function(y) mean(y-0, na.rm=T)))
}
i
print(i)
# draw Xs
X13 <- mvrnorm(n,mu=mu,Sigma=Sigma, empirical = F)
X1 <- X13[,1]
X2 <- X13[,2]
X3 <- X13[,3]
X4 <- runif(n,-3,3)
X5 <- rchisq(n, df=1)
X6 <- rbinom(n,size=1,prob=.5)
e <- rnorm(n,mean=0,sd=sqrt(30))
xb <- X1 + 2*X2 - 2*X3  - X4 - .5*X5 + X6 +  e
W  <- as.numeric(xb>0)
# sample to target sizes
sim.dat <- data.frame(W,X1,X2,X3,X4,X5,X6)
tr.keep <- sample(which(sim.dat$W==1),tr,replace=F)
co.keep <- sample(which(sim.dat$W==0),co,replace=F)
sim.dat <- sim.dat[c(tr.keep,co.keep),]
W <- sim.dat[,"W"]
X <- as.matrix(sim.dat[,names(sim.dat)[-1]])
X1 <- X[,"X1"]; X2 <- X[,"X2"]; X3 <- X[,"X3"]; X4 <- X[,"X4"]; X5 <- X[,"X5"];X6 <- X[,"X6"]
# true linear propensity scores
PS.out  <- glm(W~X,family=binomial(link = "probit"))
PS.true <- PS.out$fitted
# propensity score specification formulae
psxlist <- pslist <- list()
# ps 1
formula.PS1 <- formula(W~X1+X2+X3+X4+X5+X6)
psxlist[[1]] <- model.matrix(formula.PS1)[,-1]
# ps 2
formula.PS2  <- formula(W~I(X1^2)+I(X2^2)+X3+I(X4^2)+I(X5^2)+X6)
psxlist[[2]] <- model.matrix(formula.PS2)[,-1]
# ps 3
formula.PS3  <- formula(W~I(X1*X3)+I(X2^2)+X4+X5+X6)
psxlist[[3]] <- model.matrix(formula.PS3)[,-1]
# PS estimation
for(p in 1:3){
pslist[[p]] <- glm(W~psxlist[[p]],family=binomial(link = "probit"))
}
# outcome: treatment effect is zero
u  <- rnorm(nrow(X))
# outcome 1 (linear)
Y1 <- X1 + X2 + X3 - X4 + X5 + X6 + u
# outcome 2 (medium nonlinear)
Y2 <- X1 + X2 + 0.2*X3*X4 - sqrt(X5) + u
# outcome 3 (strongly nonlinear)
Y3 <- (X1 + X2 + X5)^2 + u
# combine
Y.mat <- cbind(Y1,Y2,Y3)
# raw outcome
for(k in 1:3){
Y <- Y.mat[,k]
est.store[[k]][i,"RAW"] <- mean(Y[W==1]) - mean(Y[W==0])
}
# Model matching predictions
mmxlist <- mmlist <- list()
for(k in 1:3){
Y <- Y.mat[,k]
# Model 1: simple linear regression on all covariates
model.MM1 <- lm(Y~X1+X2+X3+X4+X5+X6)
mmxlist[[1]] <- model.MM1$fitted
# Model 2: linear regression on all covariates with higher order polynomial terms
model.MM2 <- lm(Y~X1+I(X1^2)+I(X1^3)+X2+I(X2^2)+I(X2^3)+X3+I(X3^2)+I(X3^3)+X4+I(X4^2)+I(X4^3)+X5+I(X5^2)+I(X5^3)+X6+I(X6^2)+I(X6^3))
mmxlist[[2]] <- model.MM1$fitted
# Model 3: bagged regression trees, default 25 bags
model.MM3 <- bagging(Y~., data = data.frame(Y,X), coob=TRUE)
mmxlist[[3]] <- predict(model.MM3, X)
# Model 4: random forest
model.MM4 <- randomForest(Y~., data = data.frame(Y,X))
mmxlist[[4]] <- predict(model.MM4, X)
# combine
mmlist[[k]] <- mmxlist
}
# model-based matching
for(k in 1:4){
Y <- Y.mat[,k]
for(p in 1:4){
pred <- mmlist[[k]][[p]]
pairs <- Matches(W, pred)
pairs.df <- data.frame(do.call(rbind,pairs))
w0 <- 1/table(pairs.df[pairs.df$tr == 0,1]); Y0 <- Y[unique(pairs.df[pairs.df$tr==0,1])]
mm_res <- permu_test_mean(groups = pairs, prediction = pred, treatment = W, response = Y, iters=1000)
est.store[[k]][i,paste("MM", p, sep="")] <- mm_res[[1]] #mm_res[[1]]/tr # edit: I changed MM function to divide by number treated
est.store[[k]][i,paste("MM", p, "ATT", sep="")] <- mean(Y[W==1]) - sum(Y0*w0)/sum(w0)
}}
# model-based matching
for(k in 1:3){
Y <- Y.mat[,k]
for(p in 1:4){
pred <- mmlist[[k]][[p]]
pairs <- Matches(W, pred)
pairs.df <- data.frame(do.call(rbind,pairs))
w0 <- 1/table(pairs.df[pairs.df$tr == 0,1]); Y0 <- Y[unique(pairs.df[pairs.df$tr==0,1])]
mm_res <- permu_test_mean(groups = pairs, prediction = pred, treatment = W, response = Y, iters=1000)
est.store[[k]][i,paste("MM", p, sep="")] <- mm_res[[1]] #mm_res[[1]]/tr # edit: I changed MM function to divide by number treated
est.store[[k]][i,paste("MM", p, "ATT", sep="")] <- mean(Y[W==1]) - sum(Y0*w0)/sum(w0)
}}
rm(list=ls())
library(Matching)
library(MASS)
library(ebal)
library(devtools)
install_github("kellieotto/ModelMatch/ModelMatch")
library(ModelMatch)
library(glmnet)
library(ipred)
library(randomForest)
## Setup
sims <- 100
n    <- 5000
# X1-X3 multivariate normal with variances 2, 1, 1 and covars 1, -1, -.5
vars   <- c(2,1,1)
covars <- c(1,-1,-.5)
mu     <- c(0,0,0)
Sigma <- diag(vars)
Sigma[2,1] <- Sigma[1,2] <- covars[1]
Sigma[3,1] <- Sigma[1,3] <- covars[2]
Sigma[3,2] <- Sigma[2,3] <- covars[3]
# pick target sample size
#nsim <- 1500
#nsim <- 600
nsim  <- 300
tr <- nsim/2; tr
co <- nsim - tr; co
para  <- c("FracCtoT","N","tr","co","corY.PS","corW.PS","corW.MM1","corW.MM2","corW.MM3","corPS.PS1","corPS.PS2","corPS.PS3")
est    <- c("RAW","MM1","MM1ATT","MM2","MM2ATT","MM3","MM3ATT","MM4","MM4ATT","PS1","PS2","PS3","PSW1","PSW2","PSW3","EB","GM")
# expand storage for three different outcomes
est.store <- para.store <- list()
for(k in 1:3){
est.store[[k]]   <- matrix(NA,sims,length(est), dimnames = list(c(), est))
para.store[[k]]  <- matrix(NA,sims,length(para), dimnames = list(c(), para))
}
names(est.store) <- names(para.store) <- c("Y1","Y2","Y3")
for(i in 1:10){ # start simulations
print(i)
# draw Xs
X13 <- mvrnorm(n,mu=mu,Sigma=Sigma, empirical = F)
X1 <- X13[,1]
X2 <- X13[,2]
X3 <- X13[,3]
X4 <- runif(n,-3,3)
X5 <- rchisq(n, df=1)
X6 <- rbinom(n,size=1,prob=.5)
e <- rnorm(n,mean=0,sd=sqrt(30))
xb <- X1 + 2*X2 - 2*X3  - X4 - .5*X5 + X6 +  e
W  <- as.numeric(xb>0)
# sample to target sizes
sim.dat <- data.frame(W,X1,X2,X3,X4,X5,X6)
tr.keep <- sample(which(sim.dat$W==1),tr,replace=F)
co.keep <- sample(which(sim.dat$W==0),co,replace=F)
sim.dat <- sim.dat[c(tr.keep,co.keep),]
W <- sim.dat[,"W"]
X <- as.matrix(sim.dat[,names(sim.dat)[-1]])
X1 <- X[,"X1"]; X2 <- X[,"X2"]; X3 <- X[,"X3"]; X4 <- X[,"X4"]; X5 <- X[,"X5"];X6 <- X[,"X6"]
# true linear propensity scores
PS.out  <- glm(W~X,family=binomial(link = "probit"))
PS.true <- PS.out$fitted
# propensity score specification formulae
psxlist <- pslist <- list()
# ps 1
formula.PS1 <- formula(W~X1+X2+X3+X4+X5+X6)
psxlist[[1]] <- model.matrix(formula.PS1)[,-1]
# ps 2
formula.PS2  <- formula(W~I(X1^2)+I(X2^2)+X3+I(X4^2)+I(X5^2)+X6)
psxlist[[2]] <- model.matrix(formula.PS2)[,-1]
# ps 3
formula.PS3  <- formula(W~I(X1*X3)+I(X2^2)+X4+X5+X6)
psxlist[[3]] <- model.matrix(formula.PS3)[,-1]
# PS estimation
for(p in 1:3){
pslist[[p]] <- glm(W~psxlist[[p]],family=binomial(link = "probit"))
}
# outcome: treatment effect is zero
u  <- rnorm(nrow(X))
# outcome 1 (linear)
Y1 <- X1 + X2 + X3 - X4 + X5 + X6 + u
# outcome 2 (medium nonlinear)
Y2 <- X1 + X2 + 0.2*X3*X4 - sqrt(X5) + u
# outcome 3 (strongly nonlinear)
Y3 <- (X1 + X2 + X5)^2 + u
# combine
Y.mat <- cbind(Y1,Y2,Y3)
# raw outcome
for(k in 1:3){
Y <- Y.mat[,k]
est.store[[k]][i,"RAW"] <- mean(Y[W==1]) - mean(Y[W==0])
}
# Model matching predictions
mmxlist <- mmlist <- list()
for(k in 1:3){
Y <- Y.mat[,k]
# Model 1: simple linear regression on all covariates
model.MM1 <- lm(Y~X1+X2+X3+X4+X5+X6)
mmxlist[[1]] <- model.MM1$fitted
# Model 2: linear regression on all covariates with higher order polynomial terms
model.MM2 <- lm(Y~X1+I(X1^2)+I(X1^3)+X2+I(X2^2)+I(X2^3)+X3+I(X3^2)+I(X3^3)+X4+I(X4^2)+I(X4^3)+X5+I(X5^2)+I(X5^3)+X6+I(X6^2)+I(X6^3))
mmxlist[[2]] <- model.MM1$fitted
# Model 3: bagged regression trees, default 25 bags
model.MM3 <- bagging(Y~., data = data.frame(Y,X), coob=TRUE)
mmxlist[[3]] <- predict(model.MM3, X)
# Model 4: random forest
model.MM4 <- randomForest(Y~., data = data.frame(Y,X))
mmxlist[[4]] <- predict(model.MM4, X)
# combine
mmlist[[k]] <- mmxlist
}
# model-based matching
for(k in 1:3){
Y <- Y.mat[,k]
for(p in 1:4){
pred <- mmlist[[k]][[p]]
pairs <- Matches(W, pred)
pairs.df <- data.frame(do.call(rbind,pairs))
w0 <- 1/table(pairs.df[pairs.df$tr == 0,1]); Y0 <- Y[unique(pairs.df[pairs.df$tr==0,1])]
mm_res <- permu_test_mean(groups = pairs, prediction = pred, treatment = W, response = Y, iters=1000)
est.store[[k]][i,paste("MM", p, sep="")] <- mm_res[[1]] #mm_res[[1]]/tr # edit: I changed MM function to divide by number treated
est.store[[k]][i,paste("MM", p, "ATT", sep="")] <- mean(Y[W==1]) - sum(Y0*w0)/sum(w0)
}}
# entropy balancing
for(k in 1:3){
Y <- Y.mat[,k]
out.eb <- try(ebalance(
Treatment=W,
X=X,
print.level = -1
),silent=FALSE)
if(class(out.eb)=="try-error"){
cat("EB did not converge in sims",i,"\n")
} else {
d <- out.eb$w
est.store[[k]][i,"EB"]  <- mean(Y[W==1]) - (sum(Y[W==0]*d)/sum(d))
}}
## GenMatching
for(k in 1:3){
Y <- Y.mat[,k]
suppressWarnings(gout <- GenMatch(Tr=W, X=X, BalanceMatrix=X, estimand="ATT", M=1, weights=NULL,
print.level=0))
out  <- Match(Tr=W,X=X,Weight.matrix=gout,estimand="ATT",M=1,BiasAdjust=FALSE)
est.store[[k]][i,"GM"] <- (sum(Y[out$index.treated]*out$weights)/sum(out$weights)) - (sum(Y[out$index.control]*out$weights)/sum(out$weights))
}
# Propensity scoring methods
# Matching
for(k in 1:3){
Y <- Y.mat[,k]
for(p in 1:3){
if(sum(pslist[[p]]$fitted<(0+sqrt(.Machine$double.eps)))> 0 || sum(pslist[[p]]$fitted>(1-sqrt(.Machine$double.eps)))>0){ # error reporting if perfect sep
cat("Perfect Seperation in glm occured in sim",i,"\n")
} else {
# ps matching
PS <- pslist[[p]]$linear
out <- Match(Tr=W,X=PS,estimand="ATT",M=1,BiasAdjust = FALSE)
est.store[[k]][i,paste("PS",p,sep="")] <- (sum(Y[out$index.treated]*out$weights)/sum(out$weights)) - (sum(Y[out$index.control]*out$weights)/sum(out$weights))
}
# Weighting
PS.pr <- pslist[[p]]$fitted
d <- PS.pr/(1-PS.pr)
est.store[[k]][i,paste("PSW",p,sep="")] <- mean(Y[W==1])- (sum(Y[W==0]*d[W==0])/sum(d[W==0]))
}
# store housekeeping vars
para.store[[k]][i,"FracCtoT"] <- sum(W==0)/sum(W==1)
para.store[[k]][i,"corY.PS"] <- cor(Y,PS.true)
para.store[[k]][i,"corW.PS"] <- cor(W,PS.true)
para.store[[k]][i,"corW.MM1"] <- cor(W, mmlist[[k]][[1]])
para.store[[k]][i,"corW.MM2"] <- cor(W, mmlist[[k]][[2]])
para.store[[k]][i,"corW.MM3"] <- cor(W, mmlist[[k]][[3]])
para.store[[k]][i,"corPS.PS1"] <- cor(pslist[[1]]$fitted,PS.true)
para.store[[k]][i,"corPS.PS2"] <- cor(pslist[[2]]$fitted,PS.true)
para.store[[k]][i,"corPS.PS3"] <- cor(pslist[[3]]$fitted,PS.true)
para.store[[k]][i,"N"]  <- nrow(sim.dat)
para.store[[k]][i,"tr"] <- sum(W==1)
para.store[[k]][i,"co"] <- sum(W==0)
}
est.summ <- lapply(est.store, function(x)round(apply(x, 2, mean, na.rm=T), 5))
para.summ <- lapply(para.store, function(x)round(apply(x, 2, mean, na.rm=T), 5))
mse <- lapply(est.store, function(x) apply(x, 2, function(y) mean((y-0)^2, na.rm=T)))
bias <- lapply(est.store, function(x) apply(x, 2, function(y) mean(y-0, na.rm=T)))
}
est.summ <- lapply(est.store, function(x)round(apply(x, 2, mean, na.rm=T), 5))
para.summ <- lapply(para.store, function(x)round(apply(x, 2, mean, na.rm=T), 5))
mse <- lapply(est.store, function(x) apply(x, 2, function(y) mean((y-0)^2, na.rm=T)))
bias <- lapply(est.store, function(x) apply(x, 2, function(y) mean(y-0, na.rm=T)))
mse
bias
